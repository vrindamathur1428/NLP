{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevent libraries\n",
    "\n",
    "from selenium import webdriver \n",
    "import time \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "The \"chromedriver\", that is mandatory to run selenium library, was installed and stored in the same directory as this python file\n",
    "And it was also stored at the system PATH.\n",
    "It doesn't matter what browser you're running the notebook on, as long as you have Goggle Chrome on your Pc.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After executing this cell let the new YouTube window open and load before proceeding to next cell.\n",
    "Minimize the other other window and then proceed to execute the next cell.\n",
    "'''\n",
    "\n",
    "driver = webdriver.Chrome() \n",
    "driver.get(\"https://www.youtube.com/results?search_query=cakes\")\n",
    "\n",
    "links = []\n",
    "#this list will contain urls of all the pages we are supposed to visit\n",
    "\n",
    "df = pd.DataFrame(columns = ['link', 'title', \"no_of_views\", \"uploaded_date\", \"top_comments\"])\n",
    "#this is the dataframe in which we will store the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "this function does the following:\n",
    "1) extract the urls of all videos presently visible on the page and appends them in a list.\n",
    "2) iterates through that list visiting each page one by one.\n",
    "3) upon visiting each page, it extracts its date, views, likes and atleast one comment with the help of Explicit Wait \n",
    "& Expected Conditions class of selenium, and appends it as a new row in our dataframe.\n",
    "\n",
    "Note 1): this will take a significant amount of time to finish complete execution, \n",
    "and for that reason we are working on only around 20-25 youtube links.\n",
    "(Although if we want to iterate through more links, a few simple lines of code can be added, and small modifications can be made)\n",
    "\n",
    "Note 2): it requires having a fast internet connection, so that the pages and comments are generated timely,\n",
    "else it throws \"TimeOutException\";\n",
    "\n",
    "Note 3): it will require constant monitoring of the youtube page running in parallel.\n",
    "\"\"\"\n",
    "\n",
    "def youtube_scraping():\n",
    "    \n",
    "    wait = WebDriverWait(driver, 20)\n",
    "\n",
    "    user_data = driver.find_elements_by_xpath('//*[@id=\"video-title\"]')\n",
    "    for i in user_data:\n",
    "        links.append(i.get_attribute('href'))\n",
    "\n",
    "    for x in links:\n",
    "        driver.get(x)\n",
    "        v_link = x\n",
    "        v_title = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,\"h1.title yt-formatted-string\"))).text\n",
    "        v_views = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"yt-view-count-renderer\"))).text\n",
    "        v_date = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div#date yt-formatted-string\"))).text\n",
    "        v_comments = []\n",
    "        wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"body\"))).send_keys(Keys.END)\n",
    "        for comment in wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"#content-text\"))):\n",
    "            v_comments.append(comment.text)\n",
    "        df.loc[len(df)] = [v_link, v_title, v_views, v_date, v_comments]\n",
    "        \n",
    "'''\n",
    "P.S. Not to be a spoil sport, but extracting the number of likes and dislikes on each video was a tad bit difficult\n",
    "(because they have the same \"id\" and \"class\", as you will see if you \"inpect element\") and hence has been omitted.\n",
    "Hope to master it in future.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_scraping() #calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) #shows us how many web pages we were able to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output.csv\", index=False) #stored in the same directory as your notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
